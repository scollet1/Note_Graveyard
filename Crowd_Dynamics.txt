# **************************************************************************** #
#                                                                              #
#                                                         :::      ::::::::    #
#    Crowd_Dynamics.txt                                 :+:      :+:    :+:    #
#                                                     +:+ +:+         +:+      #
#    By: scollet <marvin@42.fr>                     +#+  +:+       +#+         #
#                                                 +#+#+#+#+#+   +#+            #
#    Created: 2017/06/22 07:54:46 by scollet           #+#    #+#              #
#    Updated: 2017/06/22 07:54:48 by scollet          ###   ########.fr        #
#                                                                              #
# **************************************************************************** #

# Crowd Dynamics
  In a crowd simulation, each agent can be represented by four vectors, and the optimal path to
    their destination can be calculated using a linear function.

  x, y : for their position on a top down, isometric view of the crowd
  s    : for their sedentation within the crowd, also known as velocity

# Defining the MDP

  S is a finite set of states
    A 2d list of possible spaces to move into. This is defined by local (x, y) coordinates, relative to the agent

  A is a finite set of actions
    A 2d list of possible movement actions. A vector to navigate and a negative or positive 1

  P_a(s, s') is the probability that action a in state s at time t will lead to s' at t + 1
    traversing the list of possible S and possible A
      if (x, y) are not occupied at s'
        then the probability of moving to, and existing in, that state is high
      else if (x, y) are occupied at s'
        then the probability of moving to, and existing in, that state is low

  R_a(s, s') is the immediate reward received after transitioning from state s to state s' due to action a
    for each step closer to s_END, a reward is given
    for each step farther from s_END, a reward is taken away

  g is an element of [0, 1] otherwise known as the discount factor
    for each step in time, rewards farthest from the agent are diminished


  Described below:
    The policy to take at state s
      for instance:
        North,
        South,
        East,
        West
    is equal to the sum over all possible next states, s'
    The probability that policy π of our current state
      times the change between our current state and each possible next state, s'
        times the reward of policy in state s
      times the change between our current state and each possible next state, s'
        plus the discounted reward of our next possible state, s'

  π(s) := ∑ P_π(s) • (s, s')(R_π(s) • (s, s') + gV(s'))
          s'

  I don't plan on accounting for future rewards just yet;
  The AI will be highly stochastic in its approach, only evaluating immediate rewards given the local search,
    but there is a possibility that some foresight can be built in

#LOCAL SEARCH NOTE
  if we measure the euclidean distance between the robot's current location and their goal,
    then we can cut our search space by only search immediately viable options
  for instance:
    if goal[x, y] is diagonal to current[x, y]
      we search the two actions which would explore the two closest s'
    else if goal[x] or goal[y] are equal to current[x] or current[y]
      we search the perpendicular s' as well as the closest s'
#END NOTE
